\documentclass[10pt,landscape]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{datetime2}
\DTMsetdatestyle{iso}

\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
    {\geometry{top=0.5cm,left=0.5cm,right=0.5cm,bottom=0.5cm}}
    {\ifthenelse{\lengthtest { \paperwidth = 11in}}
        { \geometry{top=.2in,left=.2in,right=.2in,bottom=.2in}}
        {\geometry{top=0.5cm,left=0.5cm,right=0.5cm,bottom=0.5cm}}
    }
    
\pagestyle{empty}

\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
% -----------------------------------------------------------------------

\title{MAST90104 Cheat Sheet for Enoch Ko (147938)}

\begin{document}

\raggedright
\footnotesize

%\begin{center}
%     \Large{\textbf{MAST90104 Cheat Sheet for Enoch Ko (147938)}} \\
%\end{center}
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}
Compiled: \DTMnow\\
\subsection{Linear Algebra Essentials}
\textbf{Symm.:} $\mathbf{X}^T=\mathbf{X}$;\quad 
\textbf{Ortho.:} $\mathbf{X}^T\mathbf{X}=\mathbf{I} \Rightarrow \mathbf{X}^{-1}=\mathbf{X}^T$;\\ 
$\mathbf{X}$ is ortho. matrix $\Leftrightarrow$ cols/rows form an orthonormal set.\\
\smallskip
\textbf{Idempotent:} $\mathbf{A}^2=\mathbf{A}$.\\
\hspace*{1em} Sym. idemp. $\Rightarrow$ eigs $\in\{0,1\}$, rank = trace.\\
\hspace*{1em} any 2: $ \mathbf{A}_i^2= \mathbf{A}_i$, $\sum_i A_i$ idemp., $ \mathbf{A}_i \mathbf{A}_j=0$ ($i\neq j$) $\Rightarrow$ all 3;\\
\smallskip
\textbf{Rank:} $r(\mathbf{X}) = r(\mathbf{X}^T) = r(\mathbf{X}^T \mathbf{X})$;\\
\hspace*{1em} nonsingular $\Leftrightarrow$ full rank $\Leftrightarrow \det\neq 0$.\\
\hspace*{1em} \textbf{Sylvester's Inequality:} If $\mathbf{A}$ is $m\times n$, $\mathbf{B}$ is $n\times p$: 
\hspace*{1em} $\mathrm{r}(\mathbf{A}) + \mathrm{r}(\mathbf{B}) - n \ \le \ \mathrm{r}(\mathbf{A}\mathbf{B}) \ \le \ \min\{\mathrm{r}(\mathbf{A}),\mathrm{r}(\mathbf{B})\}$.\\
\smallskip
\textbf{Trace:} $\mathrm{tr}(\mathbf{X}\mathbf{Y}) = \mathrm{tr}(\mathbf{Y}\mathbf{X})$; $\mathrm{tr}(c) = c$.\\
\hspace*{1em} $\mathrm{tr}(uv^T) = v^T u$ (vectors $u,v$).\\
\hspace*{1em} $\mathbb{E}[\mathrm{tr}(\mathbf{X})] = \mathrm{tr}[\mathbb{E}(\mathbf{X})]$; $\mathbb{E}(\mathbf{X}\mathbf{A}) = \mathbb{E}(\mathbf{X})\mathbf{A}$.\\
\smallskip
\textbf{Positive-definite:} $\mathbf{x}^T \mathbf{A} \mathbf{x} > 0$ for all $\mathbf{x}\neq 0$ \& $\mathbf{A} = \mathbf{A}^T$;\\
\hspace*{1em} SPD $\Leftrightarrow$ all eigs $>0$ (PSD $\ge0$).\\
\smallskip
\textbf{Eigenfacts:} Sym.\ $\Rightarrow$ real eigs, orthonormal eigvecs; \\
\hspace*{1em} $\exists$ ortho. $\mathbf{P}$: $\mathbf{P}^T \mathbf{A} \mathbf{P} = \mathbf{\Lambda}$ (Spectral Thm.);\\
\hspace*{1em} $\det = \prod \lambda_i$, $\mathrm{tr} = \sum\lambda_i$;\\
\hspace*{1em} Singular $\Leftrightarrow$ at least one $\lambda_i = 0 \ \Leftrightarrow \det(\mathbf{A}) = 0$.\\
\smallskip
\textbf{Simult. Diag.:} Let $\mathbf{A}_i$ be symm. $k \times k$ matrices:\\
\hspace*{1em} $\exists$ ortho. $\mathbf{P}$: $\mathbf{P}^T \mathbf{A}_i \mathbf{P} = \mathbf{\Lambda}_i (\forall i)$ $\Leftrightarrow$ $\mathbf{A}_i \mathbf{A}_j=\mathbf{A}_j\mathbf{A}_i$ ($\forall i,j$).\\
\smallskip
\textbf{Square Root:} If $\mathbf A= \mathbf P\boldsymbol\Lambda\mathbf P^T$(SPD), then $\mathbf A^{1/2}= \mathbf P\boldsymbol\Lambda^{1/2}\mathbf P^T$\\
\smallskip
\textbf{Matrix calculus:} $\frac{\partial}{\partial x}(m^T x) = m$, 
$\frac{\partial}{\partial x}(x^T \mathbf{A}) = \mathbf{A}^T$, 
$\frac{\partial}{\partial x}(x^T \mathbf{A} x) = (\mathbf{A}+\mathbf{A}^T)x$.\\
\medskip
\subsection{Finding Eigens}
Eigenvalues: $\mathrm{det}(\mathbf A-\lambda\mathbf I) = 0$, solve for $\lambda$.\\
Eigenvectors: $(\mathbf A-\lambda\mathbf I)\mathbf v=0$, solve for $ \mathbf v$ for each $\lambda$.\\
\medskip
\subsection{Determinants \& Inverses}
$\det(c\mathbf{A}) = c^n \det(\mathbf{A})$ for $n\times n$ $\mathbf{A}$; 
$\det(\mathbf{A}^k) = (\det \mathbf{A})^k$; 
$\det(\mathbf{A}\mathbf{B}) = \det(\mathbf{A})\det(\mathbf{B})$;
$\det(\mathbf {I + ab}^T) = 1+ \mathbf {a}^T \mathbf b$.\\
\medskip
$2\times 2$: $\begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}$.\\
\medskip
$\begin{vmatrix} a & b & c \\ d & e & f \\ g & h & i \end{vmatrix} \implies a\begin{vmatrix} e & f \\ h & i \end{vmatrix}-b\begin{vmatrix} d & f \\ g & i \end{vmatrix}+c\begin{vmatrix} d & e \\ g & h \end{vmatrix}$\\
\medskip
Row ops: swap$\times(-1)$, scale row$\times c$, add$\times c$$\to$det unchanged.\\
\medskip
\subsection{Partitioned Matrices}
$\mathbf{X} =
\left[ \begin{array}{c|c}
\mathbf{X}_{11} & \mathbf{X}_{12} \\
\hline
\mathbf{X}_{21} & \mathbf{X}_{22}
\end{array} \right]
\ \Rightarrow\ 
\mathbf{X}^T =
\left[ \begin{array}{c|c}
\mathbf{X}_{11}^T & \mathbf{X}_{21}^T \\
\hline
\mathbf{X}_{12}^T & \mathbf{X}_{22}^T
\end{array} \right]$\\[4pt]
\textbf{Inverse:} $\mathbf{X}^{-1} =
\left[ \begin{array}{c|c}
\tilde{\mathbf{X}}_{11}^{-1} & \tilde{\mathbf{X}}_{12}^{-1} \\
\hline
\tilde{\mathbf{X}}_{21}^{-1} & \tilde{\mathbf{X}}_{22}^{-1}
\end{array} \right]$, where:\\[4pt]
$\tilde{\mathbf{X}}_{11}^{-1} = (\mathbf{X}_{11} - \mathbf{X}_{12} \mathbf{X}_{22}^{-1} \mathbf{X}_{21})^{-1}$\\
$\tilde{\mathbf{X}}_{12}^{-1} = -\mathbf{X}_{11}^{-1} \mathbf{X}_{12} (\mathbf{X}_{22} - \mathbf{X}_{21} \mathbf{X}_{11}^{-1} \mathbf{X}_{12})^{-1}$\\
$\tilde{\mathbf{X}}_{21}^{-1} = -(\mathbf{X}_{22} - \mathbf{X}_{21} \mathbf{X}_{11}^{-1} \mathbf{X}_{12})^{-1} \mathbf{X}_{21} \mathbf{X}_{11}^{-1}$\\
$\tilde{\mathbf{X}}_{22}^{-1} = (\mathbf{X}_{22} - \mathbf{X}_{21} \mathbf{X}_{11}^{-1} \mathbf{X}_{12})^{-1}$\\
\medskip
\subsection{Linear Model}
$Y_i = \beta_0 + \sum_{j=1}^k \beta_j x_{ij} + \epsilon_i$, \quad $\mathbb{E}(\epsilon_i)=0, \mathrm{Var}(\epsilon_i)=\sigma^2$.\\
\medskip
\textbf{Simple Regression (k=1):}\\
$L(\hat{\beta}_0,\hat{\beta}_1) = \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2$\\
$\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y}}{\sum (x_i - \bar{x})^2} = \frac{\sum x_i y_i - n\bar{x}\bar{y})}{\sum x_i^2 - n\bar{x}^2}$,\quad
$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$\\
$e_i = y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i$,\quad
$\epsilon_i = y_i - \beta_0 - \beta_1 x_i$\\[4pt]
\medskip
\textbf{Matrix Form:}\\
$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon},\quad \widehat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2$.\\[2pt]
$\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$\\
$= \mathbf{y}^T \mathbf{y} - 2\mathbf{y}^T\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta}$.\\
$\frac{\partial}{\partial\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 = - 2\mathbf{y}^T \mathbf{X} + 2\boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} = 0$.\\
$\Rightarrow \widehat{\boldsymbol{\beta}}^T\mathbf{X}^T \mathbf{X}  = \mathbf{y}^T\mathbf{X}$ or $\mathbf{X}^T \mathbf{X} \widehat{\boldsymbol{\beta}} = \mathbf{X}^T \mathbf{y}$.\\
\medskip
\textbf{OLS solution:} $\widehat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}$.\\
$\mathbb{E}[\widehat{\boldsymbol{\beta}}] = \boldsymbol{\beta}$; \quad $\mathrm{Var}(\widehat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^T \mathbf{X})^{-1}$.\\
\medskip
\textbf{Estimated covariance of } $\widehat{\boldsymbol{\beta}}$: 
$\widehat{\mathrm{Var}}(\widehat{\boldsymbol{\beta}})= s^2\,(\mathbf{X}^T \mathbf{X})^{-1}\,$\\
\medskip
\textbf{Hat matrix:} $\mathbf{H} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T$, $\ \mathbf{\widehat y} = \mathbf{H}\mathbf {y}$, $\mathbf{e} = (\mathbf{I}-\mathbf{H})\mathbf{y}$.\\
$\mathbf{H}$ symm. idemp. $\Rightarrow$ $r(\mathbf{H}) = p$, $\mathrm{Var}(\mathbf{\widehat y}) = \sigma^2 \mathbf{H}$.\\
$(\mathbf{I}-\mathbf{H})$ symm. idemp., $r(\mathbf{I}-\mathbf{H}) = n-p$, $\mathrm{Var}(\mathbf{e}) = \sigma^2(\mathbf{I}-\mathbf{H})$.\\
\medskip
If $\boldsymbol\varepsilon \sim N(0,\sigma^2 \mathbf{I})$, then $\widehat{\boldsymbol{\beta}} \sim MVN\big(\boldsymbol{\beta},\ \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}\big)$.\\
\medskip
\subsection{Random Vectors}
$\mathbb{E}[\mathbf{a}] = \mathbf{a}$; $\mathbb{E}[\mathbf{a}^T \mathbf{y}] = \mathbf{a}^T \mathbb{E}[\mathbf{y}]$; $\mathbb{E}[\mathbf{A}\mathbf{y}] = \mathbf{A}\mathbb{E}[\mathbf{y}]$.\\
\medskip
$\mathrm{Var}(\mathbf{y}) = \mathbb{E}[(\mathbf{y}-\boldsymbol\mu)(\mathbf{y}-\boldsymbol\mu)^T]$, diag = variances, off-diag = covariances.\\
$\mathrm{Var}(\mathbf{y}) = V = \mathbb{E}[\mathbf{y}\mathbf{y}^T] - \boldsymbol\mu\boldsymbol\mu^T \ \Rightarrow\ \mathbb{E}[\mathbf{y}\mathbf{y}^T] = V + \boldsymbol\mu\boldsymbol\mu^T$.\\
$\mathrm{Var}(\mathbf{a}^T \mathbf{y}) = \mathbf{a}^T \mathbf{V} \mathbf{a}$; \quad $\mathrm{Var}(\mathbf{A}\mathbf{y}) = \mathbf{A}\mathbf{V}\mathbf{A}^T$.\\
The \textbf{eigenvectors} of covariance matrix = \textbf{principal components} of data -- used to find \textbf{collinear combinations} of the \textbf{predictor} variables.\\
\medskip
Example: If $\mathbf{Z} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} = \mathbf{A}\mathbf{y}$, $\mathrm{Var}(\mathbf{y}) = \sigma^2 \mathbf{I}$, then $\mathrm{Var}(\mathbf{Z}) = \mathbf{A}\mathrm{Var}(\mathbf{y})\mathbf{A}^T = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}$.\\
\medskip\medskip
\subsection{Multivariate Normal (MVN) Distribution}
$\mathbf y = \boldsymbol\mu + \boldsymbol\Sigma^{1/2} \mathbf Z,\ \mathbf{Z}\sim N(0,I) \Rightarrow \mathbf{y} \sim MVN(\boldsymbol\mu,\boldsymbol\Sigma)$\\
\medskip
Linear transform: $\mathbf{A}\mathbf{X}+\mathbf{b} \sim MVN(\mathbf{A}\boldsymbol\mu+\mathbf{b}, \mathbf{A}\boldsymbol\Sigma\mathbf{A}^T)$.\\
If $\mathbf{Z} \sim N(0, I)$, then $\mathbf{X} = \mathbf{A}\mathbf{Z} + \boldsymbol\mu \ \Rightarrow\ \mathbf{X} \sim MVN(\boldsymbol\mu, \ \mathbf{A}\mathbf{A}^T)$.
\medskip
PDF: $f(\mathbf x) = \frac{1}{(2\pi)^{k/2} |\boldsymbol\Sigma|^{1/2} } \exp \left[ -\frac{1}{2} (\mathbf x-\boldsymbol\mu)^T \boldsymbol\Sigma^{-1} (\mathbf x-\boldsymbol\mu) \right] $\\
For MVN, uncorrelated $\Leftrightarrow$ independent; in general, indep. $\Rightarrow$uncorrelated, but uncorrelated $\neq$ indep.\\
\medskip
\subsection{Random Quadratic Form}
$\mathbb E [\mathbf y^T \mathbf{Ay}] = tr(\mathbf{AV}) + \boldsymbol\mu^T \mathbf A\boldsymbol\mu$.\\
\medskip
\subsection*{Chi-Square \& QF}
Central $\chi^2_k$: $\mathbf{y} \sim N(0,\mathbf{I}_k)$, $\mathbf{y}^T\mathbf{y} \sim \chi^2_k$, $\mathbb E=k$, $\mathrm{Var}=2k$.\\
Noncentral $\chi^2_{k,\lambda}$: $\mathbf{y} \sim N(\mu,\mathbf{I}_k)$, $\lambda=\frac{1}{2}\boldsymbol\mu^T\boldsymbol\mu$, $\mathbb E=k+2\lambda$, $\mathrm{Var}=2k+8\lambda$.\\
General: $\mathbf{y} \sim N(\boldsymbol\mu,\boldsymbol{\Sigma})$ $\mapsto$ $\mathbf{y}^T\boldsymbol{\Sigma}^{-1}\mathbf{y} \sim \chi^2_{df= \mathrm{r}(\Sigma)}(\lambda=\frac{1}{2}\boldsymbol\mu^T\boldsymbol{\Sigma}^{-1}\boldsymbol\mu)$\\
\medskip
PDF:
$f(x;k,\lambda) = \sum_{i=0}^\infty \frac{e^{-\lambda}\lambda^i}{i!} g(x;k+2i)$,  
$g(x;k) = \frac{x^{k/2-1} e^{-x/2}}{2^{k/2}\Gamma(k/2)}$.\\
Sum rule: $\sum_{i=1}^n \chi^2_{k_i,\lambda_i} \sim \chi^2_{\sum k_i, \sum\lambda_i}$.  \\
\medskip
\renewcommand{\arraystretch}{1.05}
\begin{tabular}{|p{0.9cm}|p{3.3cm}|p{0.6cm}|p{1.3cm}|}
\hline
\textbf{Case} & \textbf{Conditions} & \textbf{Dist.} & \boldmath{$\lambda$} \\
\hline
Cen.\ $\mathbf{V}=\mathbf{I}$& $y\!\sim\! N(\boldsymbol 0,\mathbf{I}_n)$, $\mathbf{A}$ symm., $\mathbf{A}^2=\mathbf{A}$, $r(\mathbf{A})=k$& $\chi^2_k$ & 0 \\
\hline
Noncen.\ $\mathbf{V}=\mathbf{I}$& $y\!\sim\! N(\boldsymbol\mu,\sigma^2\mathbf{I})$, $\mathbf{A}$ symm., $\mathbf{A}^2=\mathbf{A}$, $r(\mathbf{A})=k$& $\chi^2_{k,\lambda}$ & $\frac{1}{2\sigma^2}\mu^T A\mu$ \\
\hline
Cen.\ gen.\ $\mathbf{V}$& $y\!\sim\! N(\boldsymbol 0,\mathbf{V})$, $\mathbf{A}$ symm., $({A}{V})^2 = {A}{V}$, $r({A}{V})=k$& $\chi^2_k$ & 0 \\
\hline
Noncen.\ gen.\ $\mathbf{V}$& $y\!\sim\! N(\boldsymbol\mu,\mathbf{V})$, $\mathbf{A}$ symm., $({A}{V})^2 = {A}{V}$, $r({A}{V})=k$ (add $\sigma^2$ fact. if ${V}$ scaled)& $\chi^2_{k,\lambda}$ & $\frac12\mu^T A\mu$\\
\hline
$\mathbf{V}^{-1}$& $y\!\sim\! N(\boldsymbol\mu,\mathbf{V})$, $\mathbf{V}$ nonsing.& $\chi^2_{k,\lambda}$ & $\frac12\mu^T V^{-1}\mu$ \\
\hline
\end{tabular}

\medskip
\textbf{Independence:} QF: $\mathbf{y}^T\mathbf{A}\mathbf{y}$ / LF: $\mathbf{B}\mathbf{y}$
\begin{tabular}{|p{1.3cm}|p{5.7cm}|}
\hline
QF–QF $\mathbf{V}=\mathbf{I}$& $\mathbf{A},\mathbf{B}$ symm., $\mathbf{A}\mathbf{B}=0$\\
\hline
QF–QF gen.\ $\mathbf{V}$& $\mathbf{A},\mathbf{B}$ symm.,$\mathbf{V}$ nonsing., $\mathbf{A}\mathbf{V}\mathbf{B}=0$\\
\hline
QF–LF & $\mathbf{A}$ symm., $\mathbf{B}$ ($m\times n$), $\mathbf{B}\mathbf{V}\mathbf{A}=0$\\
\hline
Multi-QF & $y\!\sim\! N(\mu,\sigma^2{I})$, ${A}_i$ symm.; any 2 of: ${A}_i^2={A}_i$,\\
& $\sum_i \mathbf{A}_i$ idemp., $\mathbf{A}_i\mathbf{A}_j=0$ ($i\neq j$) $\Rightarrow$ all 3;\\
& $\frac{1}{\sigma^2}\mathbf{y}^T \mathbf{A}_i \mathbf{y} \sim \chi^2_{r(\mathbf{A}_i),\lambda_i}$, $\lambda_i=\frac{1}{2\sigma^2}\mu^T \mathbf{A}_i \mu$;
indep.;
$\sum r(\mathbf{A}_i)=r(\sum \mathbf{A}_i)$\\
\hline
Cochran-& $y\!\sim\! N(\boldsymbol\mu,\sigma^2\mathbf{I})$, $\mathbf{A}_i$ idemp., $\sum_{i=1}^m \mathbf{A}_i=\mathbf{I}$;\\
Fisher& $\frac{1}{\sigma^2}\mathbf{y}^T \mathbf{A}_i \mathbf{y} \sim \chi^2_{r(\mathbf{A}_i),\lambda_i}$, $\lambda_i=\frac{1}{2\sigma^2}\mu^T \mathbf{A}_i \mu$, indep.; $\sum r(\mathbf{A}_i)=n$\\
\hline
\end{tabular}

\medskip
$\mathbf{A}$ Sym:
$\mathbf{y}^T \mathbf{A} \mathbf{y} \perp\!\!\!\perp \mathbf{y}^T \mathbf{B} \mathbf{y} \Leftrightarrow \mathbf{A}\mathbf{V}\mathbf{B} = 0$ ($\mathbf{V}$ non-sing.) (3.12).
$\mathbf{y}^T \mathbf{A} \mathbf{y} \perp\!\!\!\perp \mathbf{C} \mathbf{y} \Leftrightarrow \mathbf{C}\mathbf{V}\mathbf{A} = 0$ (3.14).\\
\medskip
Multiple QFs: any 2 of:$\mathbf{A}_i^2 = \mathbf{A}_i$, $\sum \mathbf{A}_i$ idem., $\mathbf{A}_i \mathbf{A}_j=0 \ (i\neq j) \Rightarrow$ all indep. $\sum r(\mathbf{A}_i)=r(\sum \mathbf{A}_i)$\\
Cochran–Fisher: $\sum \mathbf{A}_i = \mathbf{I}$, idempotent $\Rightarrow$ indep. $\chi^2$.\\
\medskip
\subsection{Gauss-Markov assumptions:}
1. True relationship between $\mathbf X$ and $\mathbf y$ is $\mathbf y= \mathbf X \boldsymbol\beta+ \boldsymbol\epsilon$, where $\mathbf X$ is a n by p matrix and $\boldsymbol\beta$ is a p-dimensional vector.\\
2. $\mathbf X$ is a full rank matrix, i.e. $\mathrm{r}( \mathbf X) = p$.\\
3. Random errors are zero-centered, i.e., $\mathbb E( \boldsymbol\epsilon)= \boldsymbol 0$$\Rightarrow$ $\mathbb E(\mathbf y)=\mathbf X \boldsymbol\beta$.\\
4. Random errors are uncorrelated, and have homogeneous variance, i.e., $\mathrm{Var}(\boldsymbol\epsilon)=\sigma^2 \mathbf I$.\\
5. Random errors are multivariate normal, i.e. $\boldsymbol\epsilon\sim\mathcal N(\boldsymbol0, \sigma^2 \mathbf I)$.\\
\medskip
Under (1-4), $\widehat{\boldsymbol\beta}=(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\mathbf y$ is BLUE of $\boldsymbol\beta$.\\
Alternate LUE $\mathbf b=\mathbf L\mathbf y=[(\mathbf X^T\mathbf X)^{-1}\mathbf X^T + \mathbf B]\mathbf y$ where $\mathbf B= \mathbf L-(\mathbf X^T\mathbf X)^{-1}\mathbf X^T$.\\
\medskip
Under (1, 5), $\mathbf y\sim MVN(\mathbf X\boldsymbol\beta, \sigma^2 \mathbf I)$. Errors $\epsilon$ are independent.
\medskip
\subsection{Maximum Likelihood Estimators}
$f(\mathbf y; \boldsymbol\beta, \sigma^2)=\frac{1}{(2\pi\sigma^2)^{n/2} }\exp\left[-\frac{\|\mathbf y-\mathbf X\boldsymbol\beta\|^2}{2\sigma^2}\right]$.\\
$\widehat{\boldsymbol{\beta}}_{\mathrm{MLE}}= (\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y} $;\quad$\widehat{\sigma}^2_{\mathrm{MLE}} = \frac{\mathrm{SS}_\mathrm{res}}{n}$;\\
$\mathrm{bias}(\widehat{\sigma}^2_{\mathrm{MLE}})=-\frac{p}{n}\sigma^2\Rightarrow$ underestimates $\sigma^2$.\\
$s^2 = \widehat{\sigma}^2_{\mathrm{unb}} = \frac{\mathrm{SS}_\mathrm{res}}{n-p}$ with $\mathbb{E}[s^2]=\sigma^2$\\
Under (1), (2), (5): $\widehat{\boldsymbol{\beta}}$ and $s^2$ are unbiased, and jointly sufficient and complete, and UMVUE for $\boldsymbol{\beta}$ and $\sigma^2$.\\
\medskip
\subsection{Proof sigma2 is biased:}
$\text{bias}=\mathbb E(\widehat{\sigma}^2)-\sigma^2=\mathbb E\left(\frac{\|\mathbf y-\mathbf X\boldsymbol\beta\|^2}{n}\right)-\sigma^2=\frac{n-p}{n}\mathbb E\left(\frac{\|\mathbf y-\mathbf X\boldsymbol\beta\|^2}{n-p}\right)-\sigma^2=\frac{n-p}{n}\mathbb E(s^2)-\sigma^2=\frac{n-p}{n}\mathbb \sigma^2-\sigma^2=-\frac{p}{n}\mathbb \sigma^2$\\
\medskip
\subsection{Proof s2 is unbiased:}
$s^2= \frac{\mathrm{SS}_\mathrm{Res} }{n-p}=\frac{n}{n-p}\widehat{\sigma}^2$;\\
$\mathbb E[s^2]=\frac{1}{n-p}\mathbb E[\|\mathbf y-\mathbf X\widehat{\boldsymbol\beta}\|^2] =\frac{1}{n-p}\mathbb E[\mathbf y^T(\mathbf I-\mathbf H)\mathbf y]=\frac{1}{n-p}\mathbb \sigma^2(n-p) = \sigma^2$
$\Leftarrow \mathbb E [\mathbf y^T \mathbf{Ay}] =\mathrm{tr}(\mathbf{AV}) + \boldsymbol\mu^T \mathbf A\boldsymbol\mu$\\
\medskip
\subsection{Diagnostics}
\textbf{Graphs:} 1. Residual vs Fitted: linearity. Large residuals$\Rightarrow$outliers; non-horizontal$\Rightarrow$missing intercept or non-linear (I); pattern$\Rightarrow$ correlated random error (IV).\\
2. QQ-plot: normality. Extreme quantile$\Rightarrow$outliers. High/low far from line$\Rightarrow$over/under-est. of tails. Asymm. over/under$\Rightarrow$skew.\\
3. Scale-Location: homoscedasticity. Unusually high $\sqrt{z_i}$$\Rightarrow$outlier. Non-horizontal trendline$\Rightarrow$non-constant error variance (IV).\\
5. Residual vs Leverage (Cook's distance). Points beyond high Cook's dist. contours$\Rightarrow$outlier.\\
\textbf{Leverage:} $h_{ii} = \frac{\partial \widehat{y}_i}{\partial y_i} = [\mathbf{H}]_{ii}$ 
\; ($0 < h_{ii} < 1$). \\
\textbf{Residual:} $e_i = y_i - \widehat{y}_i$. \\
\textbf{Var(residual):} $\mathrm{Var}(e_i) = \sigma^2 (1 - h_{ii})$. \\
\textbf{Standardised residual:} 
$z_i = \frac{e_i}{s \sqrt{1-h_{ii}}}$, \quad
$s^2 = \tfrac{\mathrm{SSE}}{n-p}$. \\
\textbf{Cook's distance:} 
$D_i = \frac{(\widehat{\boldsymbol\beta}_{(-i)}-\widehat{\boldsymbol\beta})^T\mathbf X^T\mathbf X(\widehat{\boldsymbol\beta}_{(-i)}-\widehat{\boldsymbol\beta})}{ps^2} = \frac{z_i^2}{p}\left(\frac{h_{ii} }{1-h_{ii} }\right)$, where $\widehat{\boldsymbol\beta}_{(-i)}$ is $\widehat{\boldsymbol\beta}$ if the point $i$  is removed and $p$ is the size of $\widehat{\boldsymbol\beta}$.\\
\medskip
\subsection{T Statistic}
PDF: $f(x)=\frac{\Gamma((\gamma+1)/2)}{\sqrt{\gamma\pi}\Gamma(\gamma/2)}\left(1+\frac{x^2}{\gamma}\right)^{-(\gamma+1)/2}$\\
\medskip
\subsection{Interval Estimation}
$Z=\frac{\widehat{\boldsymbol\beta}_i-\boldsymbol\beta_i}{\sigma\sqrt{c_{ii} } }\sim N(0,1)$; $X^2=\frac{(n-p)s^2}{\sigma^2}=\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2}\sim\chi^2_{n-p}$;\\
$T=\frac{\widehat{\boldsymbol\beta}_i-\boldsymbol\beta_i}{\sigma\sqrt{c_{ii} } }/\sqrt{\frac{\mathrm{SS}_\mathrm{res}/\sigma^2}{n-p} }=\frac{\widehat{\boldsymbol\beta}_i-\boldsymbol\beta_i}{\sigma\sqrt{c_{ii} } }/\sqrt{\frac{s^2}{\sigma^2}}=\frac{\widehat{\boldsymbol\beta}_i-\boldsymbol\beta_i}{s\sqrt{c_{ii} } }\sim t_{n-p}$.\\
Proof of $\widehat{\boldsymbol\beta}\perp\!\!\!\perp\frac{\mathrm{SS}_\mathrm{res}}{\sigma^2}\Leftarrow  \mathbf{B}\mathbf{V}\mathbf{A} = 0$.\\
\medskip
\subsection{Confidence Interval}
\subsubsection{100(1-alpha)\% Confidence Interval:}
$\beta_i$: CI $=\widehat{\boldsymbol\beta}_i\pm t_{\alpha/2}s\sqrt{c_{ii} }$, where $c_{ii}=[(\mathbf X^T\mathbf X)^{-1}]_{ii}$.\\
$\mathbb E(t^T \widehat{\boldsymbol\beta})$: $\text{CI} = t^T \widehat{\boldsymbol\beta}\pm t_{\alpha/2} s\sqrt{(t^T(\mathbf X^T\mathbf X)^{-1}t}$\\
$\mathbb E(y|x^\ast)$: $\text{CI} = (\mathbf x^\ast)^T\widehat{\boldsymbol\beta}\pm t_{\alpha/2} s\sqrt{(\mathbf x^\ast)^T(\mathbf X^T\mathbf X)^{-1}\mathbf x^\ast}$\\
\medskip
\subsection{Prediction Interval}
$y^\ast=(\mathbf x^\ast)^T\boldsymbol\beta+\boldsymbol\epsilon^\ast$, where $\mathrm{Var}(\boldsymbol\epsilon^\ast)=\sigma^2$ by assumption.\\
$\Rightarrow\boldsymbol\epsilon^\ast=y^\ast- (\mathbf x^\ast)^T\widehat{\boldsymbol\beta}$\\
$\because y^\ast\perp\!\!\!\perp\widehat{\boldsymbol\beta}\Leftarrow$ new vs old observations are indep.\\
$\therefore \mathrm{Var}(y^\ast- (\mathbf x^\ast)^T\widehat{\boldsymbol\beta})=\mathrm{Var}(\boldsymbol\epsilon^\ast)+\mathrm{Var}[(\mathbf x^\ast)^T\widehat{\boldsymbol\beta}]=[1+(\mathbf x^\ast)^T(\mathbf X^T\mathbf X)^{-1}\mathbf x^\ast]\sigma^2$\\
\medskip
\subsubsection{100(1-alpha)\% Prediction Interval:}
$y|x^\ast$: $\text{PI} = (\mathbf x^\ast)^T\widehat{\boldsymbol\beta}\pm t_{\alpha/2} s\sqrt{1+(\mathbf x^\ast)^T(\mathbf X^T\mathbf X)^{-1}\mathbf x^\ast}$\\
\subsection{F distribution}
Let $X^2_{\gamma_1}$ and $X^2_{\gamma_2}$ be indept. $\chi^2$ r.v. with $\gamma_1$ and $\gamma_2$ d.f., then:
$F=\frac{X^2_{\gamma_1}/\gamma_1}{X^2_{\gamma_2}/\gamma_2}$ has an F distribution with $\gamma_1$ and $\gamma_2$ d.f.\\
PDF: $f(x;\gamma_1, \gamma_2)=\frac{1}{\beta(\gamma_1/2, \gamma_2/2)}(\frac{\gamma_1}{\gamma_2})^{\gamma_1/2} x^{\gamma_1/2-1}(1+\frac{\gamma_1}{\gamma_2}x)^{-(\gamma_1+\gamma_2)/2}$\\
\medskip
\subsection{Confidence Region}
$\widehat{\boldsymbol\beta}=(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\mathbf y\sim \mathrm{MVN}(\boldsymbol\beta, (\mathbf X^T\mathbf X)^{-1}\sigma^2)$\\
$\Rightarrow \frac{(\widehat{\boldsymbol\beta}-\boldsymbol\beta)^T(\mathbf X^T\mathbf X)(\widehat{\boldsymbol\beta}-\boldsymbol\beta)}{\sigma^2}\sim \chi^2_p$; 
and $\frac{(n-p)s^2}{\sigma^2}\sim \chi^2_{n-p}$\\
\medskip
Using the F distribution formula:
$\left(\frac{(\widehat{\boldsymbol\beta}-\boldsymbol\beta)^T(\mathbf X^T\mathbf X)(\widehat{\boldsymbol\beta}-\boldsymbol\beta)}{p\sigma^2}\right)/\left(\frac{(n-p)s^2}{(n-p)\sigma^2}\right)$\\
$=\frac{(\widehat{\boldsymbol\beta}-\boldsymbol\beta)^T(\mathbf X^T\mathbf X)(\widehat{\boldsymbol\beta}-\boldsymbol\beta)}{ps^2}\sim F_{p, n-p}$\\
\medskip
$\mathbb P\left[\frac{(\widehat{\boldsymbol\beta}-\boldsymbol\beta)^T(\mathbf X^T\mathbf X)(\widehat{\boldsymbol\beta}-\boldsymbol\beta)}{ps^2} \le f_\alpha\right] = 1-\alpha$\\
Confidence Region: $(\widehat{\boldsymbol\beta}-\boldsymbol\beta)^T(\mathbf X^T\mathbf X)(\widehat{\boldsymbol\beta}-\boldsymbol\beta) \le ps^2f_\alpha$\\
CR is ellipse/ellipsoid because LHS is quadratic form whose matrix $\mathbf X^T\mathbf X$ is positive definite.\\
\medskip
\subsection{ANOVA}
\subsubsection{Sum of Squares (Total/Regression/Residual)}
$\mathrm{SS}_\mathrm{Total}=\mathbf y^T\mathbf y=\|\mathbf y\|^2=(\mathbf y-\mathbf X\widehat{\boldsymbol\beta})^T(\mathbf y-\mathbf X\widehat{\boldsymbol\beta})+(\mathbf X\widehat{\boldsymbol\beta})^T(\mathbf X\widehat{\boldsymbol\beta})=\mathrm{SS}_\mathrm{Res}+\mathrm{SS}_\mathrm{Reg}$.\\
$\mathrm{SS}_\mathrm{Reg}=\widehat{\mathbf y}^T\widehat{\mathbf y}=\mathbf y^T\mathbf H\mathbf y=(\mathbf X\widehat{\boldsymbol\beta})^T(\mathbf X\widehat{\boldsymbol\beta})=\widehat{\boldsymbol\beta}^T\mathbf X^T\mathbf X\widehat{\boldsymbol\beta}=\widehat{\mathbf y}^T\mathbf X\widehat{\boldsymbol\beta}$.\\
$\mathrm{SS}_\mathrm{Res}=(\mathbf y-\mathbf X\widehat{\boldsymbol\beta})^T(\mathbf y-\mathbf X\widehat{\boldsymbol\beta})=\mathbf y^T(\mathbf I-\mathbf H)\mathbf y$.\\
$\mathrm{SS}_\mathrm{Reg}\perp\!\!\!\perp\mathrm{SS}_\mathrm{Res}\Leftarrow\mathbf H(\mathbf I-\mathbf H)=0$\\
\medskip
df: n=sample size; p=cols in X; r=rows in L in GLH or diff. b/w models (n-p)[H1]-(n-p)[H0]; k=predictors ex-intercept.\\
\medskip
$\mathrm{MS}_\mathrm{Reg}=\frac{\mathrm{SS}_\mathrm{Reg}}{n}$;
$\mathrm{MSE}=\mathrm{MS}_\mathrm{Res}=\frac{\mathrm{SS}_\mathrm{Res}}{n-p}$.\\
\medskip
\subsubsection{Model Relevance Test:}
$H_0:\boldsymbol\beta=\boldsymbol0$;\qquad$H_1:\boldsymbol\beta\neq\boldsymbol0$.\\
$H_0:\frac{\mathrm{SS}_\mathrm{Reg} }{\sigma^2}\sim\chi^2_{k,\lambda}(\lambda=\frac{1}{2\sigma^2}\widehat{\boldsymbol\beta}^T\mathbf X^T\mathbf X\widehat{\boldsymbol\beta}=0)$;\\
Assume (I), (II), (V):\\
$f^\ast=\frac{(\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1})/p}{\mathrm{SS}_\mathrm{Res}^{H_1}/(n-p)}=\frac{\mathrm{SS}_\mathrm{Reg}^{H_1}/p}{\mathrm{SS}_\mathrm{Res}^{H_1}/(n-p)}=\frac{\mathrm{MS}_\mathrm{Reg}}{\mathrm{MS}_\mathrm{Res}}\stackrel{H_0}{\sim}F_{p, n-p}$\\
$H_1:$
$\mathbb E\left[\frac{\mathrm{SS}_\mathrm{Reg} }{p}\right] = \sigma^2+\frac{1}{p}\boldsymbol\beta^T\mathbf X^T\mathbf X\boldsymbol\beta$;
$\mathbb E\left[\frac{\mathrm{SS}_\mathrm{Res} }{n-p}\right] =\mathbb E[s^2] = \sigma^2$\\
Reject $H_0$ if $f^\ast\ge F_{\alpha;p, n-p}$; or $\text{p-value}=\mathbb P(F_{p,n-p}\ge f^\ast)\le\alpha$.\\
Reject $H_0\implies$not all coefficients are 0.\\
\medskip
\begin{tabular}{|l|l|l|l|l|}\hline
         Var Source &  $\sum$ squares & d.f.& $\sum$ mean$^2$ & F ratio\\\hline
         Regression& $\mathbf y^T\mathbf H\mathbf y$&  $p$& $\frac{\mathrm{SS}_\mathrm{Reg}}{p}$&$\frac{\mathrm{MS}_\mathrm{Reg}}{\mathrm{MS}_\mathrm{Res}}$\\\hline
         Residual&  $\mathbf y^T(\mathbf I-\mathbf H)\mathbf y$&  $n-p$&$\frac{\mathrm{SS}_\mathrm{Res}}{n-p}$& \\\hline
         Total&  $\mathbf y^T\mathbf y$&  $n$&  &\\\hline
    \end{tabular}
\medskip
\subsubsection{Intercept-Only Model Test:}
$H_0: \beta_1=\dots=\beta_k=0$; $\mathbf{y} = \boldsymbol1\beta_0 + \boldsymbol{\epsilon}$;\\
$\widehat{\beta}_0=(\boldsymbol1^T\boldsymbol1)^{-1}\boldsymbol1^T \mathbf{y}=n^{-1}\boldsymbol1\mathbf{y}=\bar{y}$.\\
$\mathrm{SS}_\mathrm{Res}^{H_0}=\mathbf y^T\mathbf y-\frac{1}{n}\mathbf y^T\boldsymbol1\boldsymbol1^T\mathbf y$.
(Corrected sum of squares)\\
\medskip
$H_1:\widehat{\boldsymbol\beta}=(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}$; $\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$.\\
$\mathrm{SS}_\mathrm{Res}^{H_1}=\mathbf y^T(\mathbf I-\mathbf H)\mathbf y$.\\
\medskip
Assume (I), (II), (V):\\
$f^\ast=\frac{(\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1})/k}{\mathrm{SS}_\mathrm{Res}^{H_1}/(n-p)}\stackrel{H_0}{\sim} F_{k, n-p}$\\
Reject $H_0$ if $f^\ast\ge F_{\alpha;k, n-p}$; or $\text{p-value}=\mathbb P(F_{k,n-p}\ge f^\ast)\le\alpha$.\\
Reject $H_0\implies$not all slopes are 0.\\
\medskip
\begin{tabular}{|l|l|l|l|l|}\hline
         Var Source &  $\sum$ squares & d.f.& $\sum$ mean$^2$ & F ratio\\\hline
         Regression& & & & \\\hline
         \hspace*{1em}Full&$R(\boldsymbol\beta)=\mathbf y^T\mathbf y$& $k+1$&&\\\hline
         \hspace*{1em}Reduced&$(\sum{y_i})^2/n$&$1$&&\\\hline
         \hspace*{1em}$\gamma_1|\gamma_2$&$R(\gamma_1|\gamma_2)$&$k$&$\frac{R(\gamma_1|\gamma_2)}{k}$&$\frac{R(\gamma_1|\gamma_2)/k}{\mathrm{MS}_\mathrm{Res}}$\\\hline
         Residual&$\mathbf y^T\mathbf y-R(\boldsymbol\beta)$&$n-k-1$&$\frac{\mathrm{SS}_\mathrm{Res}}{n-p}$& \\\hline
         Total&  $\mathbf y^T\mathbf y$&  $n$&  &\\\hline
    \end{tabular}
\medskip
Corrected Sum of Squares:
\begin{tabular}{|l|l|l|l|l|}\hline
         Var Src&  $\sum$ squares & d.f.& $\sum$ mean$^2$ & F ratio\\\hline
         Reg& $\mathrm{SS}_\mathrm{Reg}-\frac{(\sum{y_i})^2}{n}$& k& $\frac{R(\gamma_1|\gamma_2)}{k}$& $\frac{R(\gamma_1|\gamma_2)/k}{\mathrm{MS}_\mathrm{Res}}$\\\hline
         Res&$\mathrm{SS}_\mathrm{Res}$&$n-k-1$&$\frac{\mathrm{SS}_\mathrm{Res}}{n-k-1}$& \\\hline
         Total&  $\mathbf y^T\mathbf y-\frac{(\sum{y_i})^2}{n}$&  $n-1$&  &\\\hline
    \end{tabular}
\medskip
\subsubsection{General Linear Hypothesis Test:}
$H_0:\mathbf L\boldsymbol\beta=\boldsymbol\delta$;\qquad$H_1:\mathbf L\boldsymbol\beta\neq\boldsymbol\delta$\\
$\mathbf L:r\times p;\quad r=\mathrm{r(\mathbf L):r\le p}$;\quad$\boldsymbol\delta:r\times 1$ \\
\medskip
Assume (I), (II), (V) and $\mathrm{r}(L)=r\le p$:\\
$f^\ast=\frac{(\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1})/r}{\mathrm{SS}_\mathrm{Res}^{H_1}/(n-p)}\stackrel{H_0}{\sim} F_{r, n-p}$\\
Numerator of $f^\ast=\frac{1}{r}(\mathbf L\widehat{\boldsymbol\beta}-\boldsymbol\delta)^T(\mathbf{L}(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{L}^T)^{-1}(\mathbf L\widehat{\boldsymbol\beta}-\boldsymbol\delta)$\\
\medskip
Reject $H_0$ if $f^\ast\ge F_{\alpha;r, n-p}$; or $\text{p-value}=\mathbb P(F_{r,n-p}\ge f^\ast)\le\alpha$.\\
Reject $H_0\implies \mathbf L\boldsymbol\beta\neq\boldsymbol\delta$.\\
\medskip
$(\sigma^2\mathbf{L}(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{L}^T)^{-1}\sigma^2\mathbf{L}(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{L}^T=\mathbf{I}_r$\\
$\frac{\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1} }{\sigma^2}\sim\chi^2_{r(\mathbf I_r)=r, \lambda}$\\
\hspace*{4em}$\lambda=\frac{1}{2}(\mathbf L\widehat{\boldsymbol\beta}-\boldsymbol\delta)^T(\sigma^2\mathbf{L}(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{L}^T)^{-1}(\mathbf L\widehat{\boldsymbol\beta}-\boldsymbol\delta)$\\
Under $H_0:\mathrm{SS}_\mathrm{Res}^{H_1}\perp\!\!\!\perp(\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1})$.\\
\medskip
\subsubsection{Test if part of beta coefficients is zero:}
1. Split $\mathbf X$ and $\boldsymbol\beta$ into partitions: $\mathbf X=\begin{bmatrix}\mathbf X_1|\mathbf X_2\end{bmatrix}$, $\boldsymbol\beta=\begin{bmatrix}\gamma_1\\\hline\gamma_2\end{bmatrix}$.\\
2. Test: $H_0:\gamma_1=\boldsymbol0$ (reduced) vs. $H_1:\gamma_1\neq\boldsymbol{0}$ (full).\\
$\mathbf L=[\mathbf I_r|\boldsymbol0]$; $\boldsymbol\delta=\boldsymbol0$$\implies\mathbf L\boldsymbol\beta=\boldsymbol\delta\iff\gamma_1=\boldsymbol0$\\
$R(\gamma_1|\gamma_2)=(\mathbf L\widehat{\boldsymbol\beta}-\boldsymbol\delta)^T(\mathbf{L}(\mathbf{X}^T \mathbf{X})^{-1}\mathbf{L}^T)^{-1}(\mathbf L\widehat{\boldsymbol\beta}-\boldsymbol\delta)=\hat\gamma_1^T\mathbf A_{11}^{-1}\hat\gamma_1$\\
$\mathbf{A}_{11}^{-1} = \mathbf{X}_{1}^T\mathbf{X}_{1}-\mathbf{X}_{1}^T\mathbf{X}_{2}(\mathbf{X}_{2}^T\mathbf{X}_{2})^{-1}\mathbf{X}_{2}^T\mathbf{X}_{1}$\\
\medskip
$R(\beta)=\mathrm{SS}_\mathrm{Reg}^{\mathrm{full}}$;
$R(\gamma_2)=\mathrm{SS}_\mathrm{Reg}^{\mathrm{reduced}}$ (just $\mathbf X_2$);
$R(\gamma_1|\gamma_2)=R(\boldsymbol\beta)-R(\gamma_2)=\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1}$.\\
\medskip
$f^\ast=\frac{R(\gamma_1|\gamma_2)/r}{\mathrm{SS}_\mathrm{Res}/(n-p)}\sim F_{r, n-p}$\\
Reject $H_0$ if $f^\ast\ge F_{\alpha;r, n-p}$; or $\text{p-value}=\mathbb P(F_{r,n-p}\ge f^\ast)\le\alpha$.\\
\medskip
\begin{tabular}{|l|l|l|l|l|}\hline
         Var Source &  $\sum$ squares & d.f.& $\sum$ mean$^2$ & F ratio\\\hline
         Regression& & & & \\\hline
         \hspace*{1em}Full&$R(\boldsymbol\beta)$& $p$&&\\\hline
         \hspace*{1em}Reduced&$R(\gamma_2)$&$p-r$&&\\\hline
         \hspace*{1em}$\gamma_1|\gamma_2$&$R(\gamma_1|\gamma_2)$&$r$&$\frac{R(\gamma_1|\gamma_2)}{r}$&$\frac{R(\gamma_1|\gamma_2)/r}{\mathrm{MS}_\mathrm{Res}}$ \\\hline
         Residual&$\mathbf y^T\mathbf y-R(\boldsymbol\beta)$&$n-p$&$\frac{\mathrm{SS}_\mathrm{Res}}{n-p}$& \\\hline
         Total&  $\mathbf y^T\mathbf y$&  $n$&  &\\\hline
    \end{tabular}
\medskip
\subsubsection{Test if one beta coefficient is zero:}
Use $t$ test for partial test of one parameter.\\
$H_0:\beta_i=0$; $H_1:\beta_i\neq0$. (in presence of all other parameters.)\\
$f^\ast = \frac{\widehat{\boldsymbol\beta}_i^2}{c_{ii}s^2}$, equal square of $T=\frac{\widehat{\boldsymbol\beta}_i}{s\sqrt{c_{ii}}}$.\\
$f^\ast$ : T num $Z^2=\chi^2_1$, denom $(\sqrt{\chi^2/df}^2)$.\\
\medskip
\subsection{Sequential Testing}
$\frac{1}{\sigma^2}\widehat{\mathbf y}^T\widehat{\mathbf y}=\frac{1}{\sigma^2}\mathrm{SS}_\mathrm{Res}+\frac{1}{\sigma^2}R(\beta_0)+\frac{1}{\sigma^2}R(\beta_1|\beta_0)+\frac{1}{\sigma^2}R(\beta_2|\beta_0,\beta_1)+\dots+\frac{1}{\sigma^2}R(\beta_k|\beta_0,\beta_1,\dots,,\beta_k-1)$.\\
RHS qudaratic forms all indep. w/noncentral $\chi^2$, \\
$df(\mathrm{SS}_\mathrm{Res})=n-p; df(R(\beta_i|\beta_{j:k})=1\forall i,j,k$.\\
\medskip
$\mathbf A=[\mathbf A_1|\mathbf A_2]$ full rank $n\times m, n > m$ $\Rightarrow H(\mathbf A)-H(\mathbf A_i)$ idemp.\\
\medskip
\textbf{Forward Selection:} Start with empty model. Progressively add most significant variable until no more sig var to add.\\
\medskip
\textbf{Backward Elimination:} Start with full model. Remove least significant variable until no more insig var to remove.\\
\medskip
\textbf{Stepwise Selection:} Start w/any model. Add/remove 1 var based on goodness-of-fit - Akaike's Information Criterion (AIC). Smaller AIC=better. Repeat until all next step AIC greater than current model. Better than forward/backward. Finds local optimum.\\
\medskip
\subsection{Model Selection Metrics - Goodness-of-Fit}
$R^2=1-\frac{\mathrm{SS}_{\mathrm{Res}}}{\mathrm{SS}_{\mathrm{Total}}-(\sum_iy_i)^2/n)}\leftarrow$bad, always decrease.\\
$R^2_{adj}=1-\frac{n-1}{n-1-k_M}(1-R_M^2)=1-\frac{n-1}{n-p}(1-R^2)=1-\frac{n-1}{n-p}\cdot\frac{\mathrm{SS}_{\mathrm{Res}}}{\mathrm{CSS}}=1-\frac{n-1}{n-p}\cdot\frac{\mathrm{SS}_{\mathrm{Res}}}{\mathrm{SS}_{\mathrm{Total}}}$\\
\medskip
$\mathrm{AIC}=-2 \ln(L)+2p=n\ln(\frac{\mathrm{SS}_{\mathrm{Res}}}{n})+2p+const.$\\
where $L$=likelihood. Smaller AIC=better.\\
\medskip
$C_p=\frac{\mathrm{SS}_{\mathrm{Res}}(\text{model})}{s^2(\text{full model})}+2p-n$,
where p=param in intermediate model. Smaller Mallow's $C_p$=better.\\
\medskip
\subsection{Hypothesis Testing}
\begin{tabular}{|l|l|l|}\hline
& Not Significant& Significant\\\hline
$H_0$ true&True Neg ($1-\alpha$)& T1 - False Pos ($\alpha$)\\\hline
$H_1$ true&T2 - False Neg ($\beta$)& True Pos ($1-\beta$)\\ \hline
\end{tabular}
\medskip
\subsection{R Functions}
\subsubsection{General Linear Hypothesis Test:}
\texttt{delta <- c(2,0,0,1)\\
L <- diag(4)\\
linearHypothesis(model,L,delta)\\
\#\# Model 1: restricted model\\
\#\# Model 2: y$\sim$X[, -1] \# full model\\
\#\# Res.Df RSS Df Sum of Sq F Pr(>F)\\
\#\# 1 11 1788.81\\
\#\# 2 7 688.63 4 1100.2 2.7959 0.1116\\
$\mathrm{SS}_\mathrm{Res}^{H_0}=1788.81$; $\mathrm{SS}_\mathrm{Res}^{H_1}=688.63$; $r=4$; $n-p=11-4=7$;
$\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1}=1100.2$; $f^\ast=2.7959$; $\mathbf{Pr}(>f^\ast)=0.1116$.\\
\medskip
\subsubsection{Nested Model Comparisons:}
\texttt{anova(basemodel, model)\\
\#\# Model 1: area $\sim$ 0\\
\#\# Model 2: area $\sim$ midrib + estim\\
\#\# Res.Df RSS Df Sum of Sq F Pr(>F)\\
\#\# 1 139 37.806\\
\#\# 2 136 4.722 3 33.084 317.62 < 2.2e-16 ***}}\\
$\mathrm{SS}_\mathrm{Res}^{H_0}=37.806$; $\mathrm{SS}_\mathrm{Res}^{H_1}=4.722$; $r=139-136=3$; $n-p=139-3=136$;
$\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1}=4.722$; $f^\ast=317.62$; $\mathbf{Pr}(>f^\ast)=2.2 \exp(-16)$.\\
\medskip
\subsubsection{Set Beta to particular values:}
$H_0:\beta_0=-1.1; \beta_1=0.5;\beta_2=0.7$
\texttt{delta <- as.vector(c(-1.1, 0.5, 0.7))\\
h0 <- X \%*\% delta\\ \# define H0\\
basemodel <- lm(area$\sim$0, data=clover, offset=h0)\\
anova(basemodel, model)\\
\#\# Model 1: area $\sim$ 0\\
\#\# Model 2: area $\sim$ midrib + estim\\
\#\# Res.Df RSS Df Sum of Sq F Pr(>F)\\
\#\# 1 139 6.9485\\
\#\# 2 136 4.7221 3 2.2265 21.375 2.102e-11 ***}\\
$\mathrm{SS}_\mathrm{Res}^{H_0}=6.9485$; $\mathrm{SS}_\mathrm{Res}^{H_1}=4.7221$; $r=139-136=3$; $n-p=139-3=136$;
$\mathrm{SS}_\mathrm{Res}^{H_0}-\mathrm{SS}_\mathrm{Res}^{H_1}=2.2265$; $f^\ast=21.375$; $\mathbf{Pr}(>f^\ast)=2.102 \exp(-17)$.\\
\medskip
\subsubsection{Model Relevance/Set Intercept to 0:}
$H_0:\beta_0=0$
\texttt{null <- lm(area$\sim$0+midrib+estim, data=clover)\\
anova(null, model)}\\
Null model sets intercept to zero, keeps both predictors.\\
\medskip
\subsubsection{Set Beta 1 to 0:}
$H_0:\beta_1=0$
\texttt{null <- lm(area$\sim$estim, data=clover)\\
anova(null, model)}\\
Null model has intercept and $\beta_2$, but skips $\beta_1$.\\
\medskip
\subsubsection{Set Beta 2 to 0:}
$H_0:\beta_2=0$
\texttt{null <- lm(area$\sim$midrib, data=clover)\\
anova(null, model)}\\
Null model has intercept and $\beta_1$, but no $\beta_2$.\\
\medskip
\subsubsection{Sequential Testing:}
\texttt{model <- lm(area \~ midrib + estim, data=clover)\\
nm1 <- lm(area \~ 0, data=clover)\\
nm2 <- lm(area \~ 1, data=clover)\\
nm3 <- lm(area \~ midrib, data=clover)\\
anova(nm1, nm2, nm3, model)}\\
Compare nm1/2/3 vs full model.\\
\medskip
\subsubsection{Forward Selection}
\texttt{basemodel <- lm(y$\sim$1, data=heat)\\
add1(basemodel, scope=$\sim$.+x1+x2+x3+x4, test="F")}\\
Base model intercept-only. Add1 scope's (.) starts with full basemodel (int-only), then add each variable to test.\\
\medskip
\texttt{model2 <- lm(y$\sim$x4, data=heat))\\
add1(model2, scope=$\sim$.+x1+x2+x3, test="F")}\\
Model2 has int+x4. Add1 starts w/full model2, then add each variable to test.
Once add1 returns no sig vars, stop.\\
\medskip
\subsubsection{Backward Elimination}
\texttt{fullmodel <- lm(y$\sim$., data=heat)\\
drop1(fullmodel, scope=$\sim$., test="F")}\\
Start full model. Drop1 scope starts with full model, then drop each variable to test.\\
\medskip
\texttt{model2 <- lm(y$\sim$x1+x2+x4,data=heat)\\
drop1(model2, scope=$\sim$., test="F")}\\
Define model based on prev test. Drop1 then drop each variable to test. Once drop1 returns no insig vars, stop.\\
\medskip
\subsubsection{Stepwise Selection}
\texttt{model2<-step(basemodel,scope=$\sim$.+x1+x2+x3+x4,steps=1)\\
model3 <- step(model2, scope=$\sim$.+x1+x2+x3,steps=1)\\
model4 <- step(model3, scope=$\sim$.+x2+x3,steps=1)\\
step(model4, scope=$\sim$.+x3)\\
model2 <- step(fullmodel, scope=$\sim$., steps=1)\\
step(model2, scope=$\sim$.+x3)}\\
Start model. Step adds/removes each variable to test.\\
\medskip
\texttt{complete.cases()} returns a logical vector that identifies rows in a dataset that contain no missing values.
\vfill
Last updated:
2025-09-09 T01-36-58\\
Compiled: \DTMnow\\
\hrule
Enoch Ko (147934), University of Melbourne. Seat No. 1.\\
$180/100 = 1.8$ mins/mark. $1 \mapsto 1.8 // 2 \mapsto 3.6 //3 \mapsto 5.4 // 4 \mapsto 7.2 // 5 \mapsto 9 // 6 \mapsto 10.8 // 7 \mapsto 12.6// 8\mapsto 14.4$\\
$180/35 = 5$ mins/mark. $1 \mapsto 5 // 2 \mapsto 10 //3 \mapsto 15 // 4 \mapsto 20 // 5 \mapsto 25 // 6 \mapsto 30$\\
\end{multicols}
\end{document}
